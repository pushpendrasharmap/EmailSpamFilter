# -*- coding: utf-8 -*-
"""Copy of EmailSpamFilter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j0Zvu8UqDMQygDbp5uDULujL0dihqEnL
"""

from google.colab import files
upload = files.upload()

from zipfile import ZipFile

with ZipFile("SpamFilterMachineLearning-master.zip",'r') as zip:
  zip.extractall()

upload

content/

from glob import glob
import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from glob import glob
import os

nonspam_train=glob(os.path.join('/content/SpamFilterMachineLearning-master/data/nonspam-train','*.txt'))
spam_train=glob(os.path.join('/content/SpamFilterMachineLearning-master/data/spam-train','*.txt'))

nonspam_test=glob(os.path.join('/content/SpamFilterMachineLearning-master/data/nonspam-test','*.txt'))
spam_test=glob(os.path.join('/content/SpamFilterMachineLearning-master/data/spam-test','*.txt'))

all_words=[]
for i in range(len(nonspam_train)):
    file=open(nonspam_train[i],'r')
    string=file.read()
    words_in_str=string.split(' ')
    for j in range(len(words_in_str)):
        all_words.append(words_in_str[j])

for i in range(len(spam_train)):
    file=open(spam_train[i],'r')
    string=file.read()
    words_in_str=string.split(' ')
    for j in range(len(words_in_str)):
        all_words.append(words_in_str[j])

words=pd.Series(all_words) 
frequency=words.value_counts()[:2500]

print('a'+frequency.index[0]+'a')

arr_nontr=np.array([0,1,2])

for i in range(len(nonspam_train)):
    file=open(nonspam_train[i],'r')
    string=file.read()
    for j in range(len(frequency)):
        substring=' '+frequency.index[j]+' '
        count=string.count(substring)
        arr=np.array([i+1,j+1,count])
        arr_nontr=np.vstack((arr_nontr,arr))

arr_tr=np.array([0,1,2])
for i in range(len(spam_train)):
    file=open(spam_train[i],'r')
    string=file.read()
    for j in range(len(frequency)):
        substring=' '+frequency.index[j]+' '
        count=string.count(substring)
        arr=np.array([i+1,j+1,count])
        arr_tr=np.vstack((arr_tr,arr))

arr_nonte=np.array([0,1,2])
for i in range(len(nonspam_test)):
    file=open(nonspam_test[i],'r')
    string=file.read()
    for j in range(len(frequency)):
        substring=' '+frequency.index[j]+' '
        count=string.count(substring)
        arr=np.array([i+1,j+1,count])
        arr_nonte=np.vstack((arr_nonte,arr))

print(arr_nonte)

arr_te=np.array([0,1,2])
for i in range(len(spam_test)):
    file=open(spam_test[i],'r')
    string=file.read()
    for j in range(len(frequency)):
        substring=' '+frequency.index[j]+' '
        count=string.count(substring)
        arr=np.array([i+1,j+1,count])
        arr_te=np.vstack((arr_te,arr))

print(arr_nontr[1:,[1,2]])

labels_nontr=np.zeros((350,))
labels_tr=np.ones((350,))
labels_nonte=np.zeros((130,))
labels_te=np.ones((130,))

a1=arr_nontr[1:,[2]].reshape((350,2500))
a2=arr_tr[1:,[2]].reshape((350,2500))
a3=arr_nonte[1:,[2]].reshape((130,2500))
a4=arr_te[1:,[2]].reshape((130,2500))

print(a1)

arr_train=np.vstack((a1,a2))
arr_test=np.vstack((a3,a4))

print(arr_test)

labels_train=np.append(labels_nontr,labels_tr)
labels_test=np.append(labels_nonte,labels_te)
print(labels_test)

from keras import layers

from keras import models

model=models.Sequential()

model.add(layers.Dense(64,activation='relu',input_shape=(2500,)))
model.add(layers.Dense(32,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))

model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])

model.fit(arr_train,labels_train,batch_size=32,epochs=20)

a,b=model.evaluate(arr_test,labels_test)

print(a)
print(b)